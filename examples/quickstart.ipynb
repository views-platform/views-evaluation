{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "In this notebook, we go over the main functionalities of the library\n",
    "\n",
    "## Table of Contents\n",
    "1. [Installation](#installation)\n",
    "2. [Importing Libraries](#importing-libraries)\n",
    "3. [Creating Sample Data](#creating-sample-data)\n",
    "4. [Initializing EvaluationManager](#initializing-evaluationmanager)\n",
    "5. [Evaluating Predictions](#evaluating-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using some virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install views-evaluation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "First, let's import a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from views_evaluation.evaluation.evaluation_manager import EvaluationManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Data\n",
    "\n",
    "Let's create some sample data for actual values and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(\n",
    "    [(99, 1), (99, 2), (100, 1), (100, 2), (101, 1), (101, 2), (102, 1), (102, 2)],\n",
    "    names=[\"month\", \"country\"],\n",
    ")\n",
    "index_0 = pd.MultiIndex.from_tuples(\n",
    "    [(100, 1), (100, 2), (101, 1), (101, 2)],\n",
    "    names=[\"month\", \"country\"],\n",
    ")\n",
    "index_1 = pd.MultiIndex.from_tuples(\n",
    "    [(101, 1), (101, 2), (102, 1), (102, 2)],\n",
    "    names=[\"month\", \"country\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual data\n",
    "df_actual = pd.DataFrame(\n",
    "    {\n",
    "        \"lr_target\": [0, 1, 1, 2, 2, 3, 3, 4],\n",
    "        \"covariate_1\": [3, 2, 4, 5, 2, 6, 8, 5],\n",
    "    },\n",
    "    index=index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point predictions\n",
    "df1_point = pd.DataFrame({\"pred_lr_target\": [1, 3, 5, 7]}, index=index_0)\n",
    "df2_point = pd.DataFrame({\"pred_lr_target\": [2, 4, 6, 8]}, index=index_1)\n",
    "dfs_point = [df1_point, df2_point]\n",
    "\n",
    "# Uncertainty\n",
    "df1_uncertainty = pd.DataFrame(\n",
    "    {\"pred_lr_target\": [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]}, index=index_0\n",
    ")\n",
    "df2_uncertainty = pd.DataFrame(\n",
    "    {\"pred_lr_target\": [[4, 6, 8], [5, 7, 9], [6, 8, 10], [7, 9, 11]]}, index=index_1\n",
    ")\n",
    "dfs_uncertainty = [df1_uncertainty, df2_uncertainty]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing EvaluationManager\n",
    "Now, we can initialize the `EvaluationManager` with the metrics we want to evaluate.\n",
    "Point evaluation supports the following metrics:\n",
    "- RMSLE\n",
    "- CRPS\n",
    "- Average Precision\n",
    "\n",
    "Uncertainty evaluation supports the following metric:\n",
    "- CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = ['RMSLE', 'CRPS', 'MIS'] # Add other metrics as needed\n",
    "evaluation_manager = EvaluationManager(metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric MIS is not a default metric, skipping...\n",
      "Metric MIS is not a default metric, skipping...\n",
      "Metric MIS is not a default metric, skipping...\n"
     ]
    }
   ],
   "source": [
    "steps = [1, 2]\n",
    "point_evaluation_results = evaluation_manager.evaluate(df_actual, dfs_point, target='lr_target', steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             RMSLE  CRPS\n",
       " month100  0.203422   0.5\n",
       " month101  0.502668   2.0\n",
       " month102  0.573874   3.5,\n",
       "            RMSLE  CRPS\n",
       " step01  0.182040   0.5\n",
       " step02  0.636311   3.5,\n",
       "          RMSLE  CRPS\n",
       " ts00  0.510800   2.0\n",
       " ts01  0.420849   2.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_evaluation_results['month'][1], point_evaluation_results['step'][1], point_evaluation_results['time_series'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics will be **ignored** if not in the supported metric list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric RMSLE is not a default metric, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric RMSLE is not a default metric, skipping...\n",
      "Metric RMSLE is not a default metric, skipping...\n"
     ]
    }
   ],
   "source": [
    "uncertainty_evaluation_results = evaluation_manager.evaluate(df_actual, dfs_uncertainty, target='lr_target', steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              CRPS     MIS\n",
       " month100  0.555556    3.90\n",
       " month101  2.333333   65.85\n",
       " month102  4.111111  127.80,\n",
       "             CRPS    MIS\n",
       " step01  1.833333  45.85\n",
       " step02  2.833333  85.85,\n",
       "           CRPS    MIS\n",
       " ts00  1.055556   23.9\n",
       " ts01  3.611111  107.8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_evaluation_results['month'][1], uncertainty_evaluation_results['step'][1], uncertainty_evaluation_results['time_series'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are only interested in one of the evaluation schemas, you can call the corresponding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the evaluation type, i.e., uncertainty or point\n",
    "actual = EvaluationManager.transform_data(\n",
    "            EvaluationManager.convert_to_arrays(df_actual), 'lr_target'\n",
    "        )\n",
    "predictions = [\n",
    "    EvaluationManager.transform_data(\n",
    "        EvaluationManager.convert_to_arrays(pred), f\"pred_lr_target\"\n",
    "    )\n",
    "    for pred in dfs_point\n",
    "]\n",
    "is_uncertainty = EvaluationManager.get_evaluation_type(predictions)\n",
    "month_point_evaluation_results = evaluation_manager.month_wise_evaluation(actual, predictions, target='lr_target', is_uncertainty=is_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             RMSLE  CRPS\n",
      "month100  0.203422   0.5\n",
      "month101  0.502668   2.0\n",
      "month102  0.573874   3.5\n"
     ]
    }
   ],
   "source": [
    "print(month_point_evaluation_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
